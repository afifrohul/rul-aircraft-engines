{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUtUBGNralax"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'FD001' # subset data yang ingin digunakan\n",
    "n_xgb = '10' # jumlah fitur terbaik yang ingin dipilih\n",
    "now = datetime.now()\n",
    "formatted = now.strftime(\"%d-%m-%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06BOTz9Lalaz"
   },
   "source": [
    "# Data Train Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "UUXuEsVwalaz",
    "outputId": "8b3fd3d6-79de-4e1c-e137-85d25c0f08e7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../CMAPSSData/train_' + subset + '.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "53Rt9xR-ala0",
    "outputId": "02c557fe-0e22-4069-931c-f39097e6e49c"
   },
   "outputs": [],
   "source": [
    "def piecewise_linear_rul(df, rul_max=128):\n",
    "    def compute_rul(cycles):\n",
    "        max_cycle = cycles.max()\n",
    "        rul = max_cycle - cycles\n",
    "        return np.where(rul > rul_max, rul_max, rul)\n",
    "\n",
    "    df['RUL'] = df.groupby('unit_number')['cycles'].transform(compute_rul)\n",
    "    return df\n",
    "\n",
    "piecewise_linear_rul(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dtc45wLWXVxZ"
   },
   "outputs": [],
   "source": [
    "X_xgb = df.drop(columns=['unit_number', 'cycles', 'RUL'])\n",
    "y_xgb = df['RUL'] + 1\n",
    "\n",
    "print(np.isnan(X_xgb).any()) \n",
    "print(np.isnan(y_xgb).any()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "_UXE5RuOUAbk",
    "outputId": "fa4fef70-1fd6-4e7f-9265-13b71f0dbec3"
   },
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    objective='reg:gamma',\n",
    "    gamma=0.1,   \n",
    "    reg_lambda=3,\n",
    "    subsample=0.7,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_child_weight=7, \n",
    "    n_estimators=250\n",
    ")\n",
    "xgb_model.fit(X_xgb, y_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDg6qpCad3ye"
   },
   "outputs": [],
   "source": [
    "features = [col for col in df.columns if col not in ['unit_number', 'cycles', 'RUL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g50A8ikrX2vA",
    "outputId": "5f55b1d1-3467-47d6-c8a4-daa8acc21fb7"
   },
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(xgb_model.feature_importances_)[::-1]\n",
    "top_n = int(n_xgb)  # Jumlah fitur terbaik yang diinginkan\n",
    "\n",
    "# Ambil fitur terbaik\n",
    "selected_features = np.array(features)[sorted_indices[:top_n]]\n",
    "\n",
    "print(\"Fitur yang terpilih:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpt3QN39oQ5b"
   },
   "outputs": [],
   "source": [
    "features = selected_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "PNYHIKomf_ao",
    "outputId": "2302e9a1-ecbf-4b55-fdfe-32f6c3bfe709"
   },
   "outputs": [],
   "source": [
    "df_selected = df[['unit_number', 'cycles'] + selected_features.tolist() + ['RUL']].copy()\n",
    "df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "t2UE9oCBala0",
    "outputId": "0f2ea3d3-6376-4ae0-a1af-966a8f2a117d"
   },
   "outputs": [],
   "source": [
    "# Normalisasi data\n",
    "mean = df_selected[selected_features].mean()\n",
    "std = df_selected[selected_features].std()\n",
    "\n",
    "# Terapkan normalisasi\n",
    "df_selected[selected_features] = (df_selected[selected_features] - mean) / std\n",
    "df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qorbGxVNala1",
    "outputId": "87d4be2b-9c73-46f9-cb8d-435e9524e206"
   },
   "outputs": [],
   "source": [
    "# Sliding window\n",
    "window_size=39\n",
    "def create_sliding_window(data, window_size=50, stride=1):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    for unit in data['unit_number'].unique():\n",
    "        unit_data = data[data['unit_number'] == unit]\n",
    "        for i in range(0, len(unit_data) - window_size + 1, stride):\n",
    "            windows.append(unit_data.iloc[i:i+window_size][features].values)\n",
    "            labels.append(unit_data.iloc[i+window_size-1]['RUL'])\n",
    "    return np.array(windows), np.array(labels)\n",
    "\n",
    "X, y = create_sliding_window(df_selected, window_size=window_size, stride=1)\n",
    "\n",
    "\n",
    "print(np.isnan(X).any())  # Periksa NaN pada X\n",
    "print(np.isinf(X).any())  # Periksa Infinity pada X\n",
    "print(np.isnan(y).any())  # Periksa NaN pada y\n",
    "print(np.isinf(y).any())  # Periksa Infinity pada y\n",
    "\n",
    "print(f\"Shape of input data: {X.shape}\")\n",
    "print(f\"Shape of labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2eWhzK6zala1",
    "outputId": "1e041d18-3a5b-4caa-c90e-035c397d3bad"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pisahkan data train menjadi train set dan validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=237)\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}\")\n",
    "print(f\"Shape y_train: {y_train.shape}\")\n",
    "print(f\"Shape X_val: {X_val.shape}\")\n",
    "print(f\"Shape y_val: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul6THDHsy-Ys"
   },
   "source": [
    "# Data Test Procces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "FubKbPyK1USi",
    "outputId": "1d8d87c4-c9e8-4ec3-a54f-b160c1c29513"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../CMAPSSData/test_' + subset + '.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(df_test).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "zXSAdJ952P7D",
    "outputId": "a6927bd5-6132-4586-a4f9-5f3699e9298c"
   },
   "outputs": [],
   "source": [
    "df_test_selected = df_test[['unit_number', 'cycles'] + selected_features.tolist()].copy()\n",
    "df_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "w0GBddmo25cj",
    "outputId": "b9749e31-757a-44a7-a111-b74274bc14b7"
   },
   "outputs": [],
   "source": [
    "# Terapkan normalisasi\n",
    "df_test_selected[selected_features] = (df_test_selected[selected_features] - mean) / std\n",
    "df_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aF-MUZT6hQB",
    "outputId": "d869d296-d710-4527-c1c9-c0fe29fa2c2e"
   },
   "outputs": [],
   "source": [
    "def pad_window(data, window_size, features):\n",
    "    if len(data) < window_size:\n",
    "        padding = np.zeros((window_size - len(data), len(features)))  # Padding dengan nol\n",
    "        padded_data = np.vstack((padding, data))\n",
    "    else:\n",
    "        padded_data = data[-window_size:]  # Ambil window terakhir\n",
    "    return padded_data\n",
    "\n",
    "X_test = []\n",
    "for unit in df_test_selected['unit_number'].unique():\n",
    "    unit_data = df_test_selected[df_test_selected['unit_number'] == unit][features].values\n",
    "    test_window = pad_window(unit_data, window_size=window_size, features=features)\n",
    "    X_test.append(test_window)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWq_zEgm7Ow8"
   },
   "source": [
    "# RUL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "LBFB85eS7RaN",
    "outputId": "6e298c0b-3a9d-4292-e92c-b847615d84ad"
   },
   "outputs": [],
   "source": [
    "y_actual = pd.read_csv('../CMAPSSData/RUL_' + subset + '.csv')\n",
    "y_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMRiGf-37VGa"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NFy5WFV7Z5c"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, seed):\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=seed)\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0.0, input_shape=input_shape))\n",
    "    model.add(LSTM(32, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(16, activation='relu', kernel_initializer=initializer))\n",
    "    model.add(Dense(1, kernel_initializer=initializer))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOh0hvoN7bgE"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "n_models = 1 # Jumlah model dalam ensemble. 1 = Model tunggal, >1 = Ensemble (15 atau dst)\n",
    "input_shape = (window_size, len(features))\n",
    "ensemble_models = []\n",
    "ensemble_histories = []\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001,\n",
    "                                start_from_epoch=85, restore_best_weights=True)\n",
    "\n",
    "for i in range(n_models):\n",
    "    print(f\"Training model {i+1}/{n_models}\")\n",
    "    model = create_lstm_model(input_shape, seed=237 + i)\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    ensemble_models.append(model)\n",
    "    ensemble_histories.append(history)\n",
    "    model.save(f'models_{n_models}_{subset}_{n_xgb}xgboost_{formatted}/models/ensemble_model_{i}.h5')  # Simpan model individu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Jumlah model dalam ensemble\n",
    "num_models = len(ensemble_histories)\n",
    "\n",
    "# Tentukan jumlah baris dan kolom (2 plot per baris)\n",
    "cols = 5\n",
    "rows = math.ceil(num_models / cols)  # Hitung jumlah baris yang dibutuhkan\n",
    "\n",
    "# Buat subplot untuk setiap model\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))  # Ukuran subplot\n",
    "axes = axes.flatten()  # Flatten array axes untuk akses mudah\n",
    "\n",
    "# Plot loss untuk setiap model\n",
    "for i, history in enumerate(ensemble_histories):\n",
    "    ax = axes[i]\n",
    "    ax.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss', color='orange', linestyle='--')\n",
    "    ax.set_title(f'Model {i+1}', fontsize=12)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hapus axes kosong jika jumlah model kurang dari jumlah subplot\n",
    "for j in range(len(axes)):\n",
    "    if j >= num_models:\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "# Tambahkan judul utama dan tata letak\n",
    "fig.suptitle('Training and Validation Loss for Ensemble Models', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Sesuaikan ruang untuk judul utama\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights):\n",
    "        super(WeightedEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)  # Ganti nama dari self.weights\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = tf.stack([model(inputs, training=False) for model in self.models], axis=0)\n",
    "        return tf.tensordot(self.model_weights, outputs, axes=1)\n",
    "    \n",
    "# Hitung bobot berdasarkan val_loss\n",
    "weights = []\n",
    "valid_models = []\n",
    "valid_histories = []\n",
    "for model, history in zip(ensemble_models, ensemble_histories):\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    if not np.isnan(final_val_loss):\n",
    "        weights.append(1 / final_val_loss)\n",
    "        valid_models.append(model)\n",
    "        valid_histories.append(history)\n",
    "\n",
    "normalized_weights = np.array(weights) / np.sum(weights)\n",
    "ensemble_combined_model = WeightedEnsemble(valid_models, normalized_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict_probabilistic(models, X, histories):\n",
    "    valid_models = []\n",
    "    weights = []\n",
    "    for model, history in zip(models, histories):\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        if not np.isnan(final_val_loss):\n",
    "            valid_models.append(model)\n",
    "            weights.append(1 / final_val_loss)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    predictions = [model.predict(X) for model in valid_models]\n",
    "    weighted_predictions = np.average(predictions, axis=0, weights=normalized_weights)\n",
    "    std_predictions = np.sqrt(np.average((predictions - weighted_predictions) ** 2,\n",
    "                                         axis=0, weights=normalized_weights))\n",
    "    return weighted_predictions, std_predictions\n",
    "\n",
    "def calculate_rmse(y_actual, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "\n",
    "def calculate_picp(y_actual, mean_pred, delta):\n",
    "    lower_bound = mean_pred - delta\n",
    "    upper_bound = mean_pred + delta\n",
    "    within_interval = np.logical_and(y_actual >= lower_bound, y_actual <= upper_bound)\n",
    "    return np.mean(within_interval)\n",
    "\n",
    "def calculate_nmpiw(y_actual, delta):\n",
    "    interval_width = 2 * delta\n",
    "    return np.mean(interval_width) / (np.max(y_actual) - np.min(y_actual))\n",
    "\n",
    "mean_pred, std_pred = ensemble_predict_probabilistic(ensemble_models, X_test, ensemble_histories)\n",
    "z = 1.96\n",
    "delta = z * std_pred\n",
    "\n",
    "rmse = calculate_rmse(y_actual, mean_pred)\n",
    "picp = calculate_picp(y_actual, mean_pred, delta)\n",
    "nmpiw = calculate_nmpiw(y_actual, delta)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"PICP: {picp}\")\n",
    "print(f\"NMPIW: {nmpiw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model gabungan\n",
    "\n",
    "ensemble_combined_model.save(f'models_{n_models}_{subset}_{n_xgb}xgboost_{formatted}/{round(rmse, 4)}_{round(picp, 4)}_{round(nmpiw, 4)}_ensemble_model_combined.h5')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
