{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1732769151541,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "bUtUBGNralax"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Conv1D, MaxPooling1D, UpSampling1D, Input, Masking, Flatten, Reshape\n",
    ")\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'FD001' # subset data yang ingin digunakan\n",
    "now = datetime.now()\n",
    "formatted = now.strftime(\"%d-%m-%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06BOTz9Lalaz"
   },
   "source": [
    "# Data Train Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732769152070,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "UUXuEsVwalaz",
    "outputId": "0688eec1-b28b-448c-8bfd-a2e451013564"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../CMAPSSData/train_' + subset + '.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732769152071,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "ltu0HXRJala0",
    "outputId": "53c5471d-7918-476b-c80e-eebf99def38e"
   },
   "outputs": [],
   "source": [
    "drop_sensor = ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sm_1', 'sm_5', 'sm_6', 'sm_8', 'sm_10', 'sm_13', 'sm_15', 'sm_16', 'sm_18', 'sm_19']\n",
    "df = df.drop(columns=drop_sensor)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1732769152596,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "53Rt9xR-ala0",
    "outputId": "6f520cec-c9ac-4d40-ea36-87bf330aeb82"
   },
   "outputs": [],
   "source": [
    "def piecewise_linear_rul(df, rul_max=125):\n",
    "    def compute_rul(cycles):\n",
    "        max_cycle = cycles.max()\n",
    "        rul = max_cycle - cycles\n",
    "        return np.where(rul > rul_max, rul_max, rul)\n",
    "\n",
    "    df['RUL'] = df.groupby('unit_number')['cycles'].transform(compute_rul)\n",
    "    return df\n",
    "\n",
    "piecewise_linear_rul(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732769152597,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "t2UE9oCBala0",
    "outputId": "8463cbbb-53c8-4165-a052-368d63c2eca5"
   },
   "outputs": [],
   "source": [
    "# Normalisasi data\n",
    "features = [col for col in df.columns if col not in ['unit_number', 'cycles', 'RUL']]\n",
    "mean = df[features].mean()\n",
    "std = df[features].std()\n",
    "\n",
    "df[features] = (df[features] - mean) / std\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15748,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "qorbGxVNala1",
    "outputId": "d229f85a-2162-47a6-f9ca-654a20829baa"
   },
   "outputs": [],
   "source": [
    "# Sliding window\n",
    "window_size = 39\n",
    "def create_sliding_window(data, window_size=50, stride=1):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    for unit in data['unit_number'].unique():\n",
    "        unit_data = data[data['unit_number'] == unit]\n",
    "        for i in range(0, len(unit_data) - window_size + 1, stride):\n",
    "            windows.append(unit_data.iloc[i:i+window_size][features].values)\n",
    "            labels.append(unit_data.iloc[i+window_size-1]['RUL'])\n",
    "    return np.array(windows), np.array(labels)\n",
    "\n",
    "X, y = create_sliding_window(df, window_size=window_size, stride=1)\n",
    "\n",
    "\n",
    "print(np.isnan(X).any())  # Periksa NaN pada X\n",
    "print(np.isinf(X).any())  # Periksa Infinity pada X\n",
    "print(np.isnan(y).any())  # Periksa NaN pada y\n",
    "print(np.isinf(y).any())  # Periksa Infinity pada y\n",
    "\n",
    "print(f\"Shape of input data: {X.shape}\")\n",
    "print(f\"Shape of labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "2eWhzK6zala1",
    "outputId": "d3395bae-e3c1-46d1-a223-d7d20a2ca303"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pisahkan data train menjadi train set dan validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=237)\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}\")\n",
    "print(f\"Shape y_train: {y_train.shape}\")\n",
    "print(f\"Shape X_val: {X_val.shape}\")\n",
    "print(f\"Shape y_val: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7L9TqS0ala1"
   },
   "source": [
    "# Data Test Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "VpHDq5SZala1",
    "outputId": "4aad77f3-ee7b-4861-c2e1-2602664ab397"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../CMAPSSData/test_' + subset + '.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "FTNUXCNYala1",
    "outputId": "e3d699e9-8644-497d-91f0-aa94ee596fdd"
   },
   "outputs": [],
   "source": [
    "drop_sensor = ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sm_1', 'sm_5', 'sm_6', 'sm_8', 'sm_10', 'sm_13', 'sm_15', 'sm_16', 'sm_18', 'sm_19']\n",
    "df_test = df_test.drop(columns=drop_sensor)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "3qy0M3lqala1",
    "outputId": "bad79c11-7b87-45ff-c394-c8e4a07b8599"
   },
   "outputs": [],
   "source": [
    "# Normalisasi data\n",
    "features = [col for col in df_test.columns if col not in ['unit_number', 'cycles', 'RUL']]\n",
    "\n",
    "df_test[features] = (df_test[features] - mean) / std\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "9RaXNQYOala1",
    "outputId": "3db54ebb-c1b0-4fb9-e22f-b48e3736569e"
   },
   "outputs": [],
   "source": [
    "def pad_window(data, window_size, features):\n",
    "    if len(data) < window_size:\n",
    "        padding = np.zeros((window_size - len(data), len(features)))  # Padding dengan nol\n",
    "        padded_data = np.vstack((padding, data))\n",
    "    else:\n",
    "        padded_data = data[-window_size:]  # Ambil window terakhir\n",
    "    return padded_data\n",
    "\n",
    "X_test = []\n",
    "for unit in df_test['unit_number'].unique():\n",
    "    unit_data = df_test[df_test['unit_number'] == unit][features].values\n",
    "    test_window = pad_window(unit_data, window_size=window_size, features=features)\n",
    "    X_test.append(test_window)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "it81kp8Sala1",
    "outputId": "46dc17ca-b5eb-4cd8-aecc-18100f86226b"
   },
   "outputs": [],
   "source": [
    "print(np.isnan(X_test).any())  # Periksa NaN pada X_test\n",
    "print(np.isinf(X_test).any())  # Periksa Infinity pada X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_upsVPInala2"
   },
   "source": [
    "# RUL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "SodCoNcQala2",
    "outputId": "de7396e7-55ce-4d33-e218-70dda0231815"
   },
   "outputs": [],
   "source": [
    "y_actual = pd.read_csv('../CMAPSSData/RUL_' + subset + '.csv')\n",
    "y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "8GVTyTovala2",
    "outputId": "fd5621d2-0fc5-4065-ac04-da298bb6cd57"
   },
   "outputs": [],
   "source": [
    "print(np.isnan(y_actual['RUL']).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJkizgReala2"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1732769168339,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "5-V08vUBala2"
   },
   "outputs": [],
   "source": [
    "def create_cnn_autoencoder_lstm_model(input_shape, seed):\n",
    "    initializer = GlorotUniform(seed=seed)\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Masking untuk nilai padding\n",
    "    x = Masking(mask_value=0.0)(input_layer)\n",
    "    \n",
    "    # CNN Encoder: Mengekstraksi fitur spasial\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)  # Downsampling\n",
    "    \n",
    "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    \n",
    "    # CNN Decoder: Merekonstruksi fitur spasial\n",
    "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(x)\n",
    "    x = UpSampling1D(size=2)(x)  # Upsampling\n",
    "    \n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(x)\n",
    "    x = UpSampling1D(size=2)(x)\n",
    "    \n",
    "    # Output CNN langsung dikirim ke LSTM dengan mengontrol shape-nya\n",
    "    x = Conv1D(filters=64, kernel_size=1, activation='relu', padding='same', kernel_initializer=initializer)(x)\n",
    "    \n",
    "    # LSTM Layer untuk menangkap pola temporal\n",
    "    x = LSTM(32, activation='relu', return_sequences=True, kernel_initializer=initializer)(x)\n",
    "    x = LSTM(16, activation='relu', return_sequences=False, kernel_initializer=initializer)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    output = Dense(1, activation='linear', kernel_initializer=initializer)(x)  # Tidak perlu slicing\n",
    "\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999), loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E36Tc3Dala2"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 566887,
     "status": "error",
     "timestamp": 1732772388350,
     "user": {
      "displayName": "Lytaa Panda",
      "userId": "02950352217214470042"
     },
     "user_tz": -420
    },
    "id": "yV0oPjWEala2",
    "outputId": "d253d9ee-8f66-4c04-f9fa-14adccc23cd8"
   },
   "outputs": [],
   "source": [
    "# Parameter pelatihan\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "n_models = 1  # Jumlah model dalam ensemble, 1 = Model tunggal, >1 = Ensemble (15 atau dst)\n",
    "\n",
    "# List untuk menyimpan model ensemble\n",
    "ensemble_models = []\n",
    "ensemble_histories = []  # Untuk menyimpan history\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001,\n",
    "                                start_from_epoch=85, restore_best_weights=True)\n",
    "\n",
    "# lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Buat dan latih model\n",
    "input_shape = (window_size, len(features))  # (window_size, jumlah fitur)\n",
    "for i in range(n_models):\n",
    "    print(f\"Training model {i+1}/{n_models}\")\n",
    "    model = create_cnn_autoencoder_lstm_model(input_shape, seed=237 + i)\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "    ensemble_models.append(model)\n",
    "    ensemble_histories.append(history)\n",
    "    model.save(f'models_{n_models}_{subset}_{formatted}/models/ensemble_model_{i}.h5')  # Simpan model individu\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Jumlah model dalam ensemble\n",
    "num_models = len(ensemble_histories)\n",
    "\n",
    "# Tentukan jumlah baris dan kolom (2 plot per baris)\n",
    "cols = 5\n",
    "rows = math.ceil(num_models / cols)  # Hitung jumlah baris yang dibutuhkan\n",
    "\n",
    "# Buat subplot untuk setiap model\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))  # Ukuran subplot\n",
    "axes = axes.flatten()  # Flatten array axes untuk akses mudah\n",
    "\n",
    "# Plot loss untuk setiap model\n",
    "for i, history in enumerate(ensemble_histories):\n",
    "    ax = axes[i]\n",
    "    ax.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss', color='orange', linestyle='--')\n",
    "    ax.set_title(f'Model {i+1}', fontsize=12)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hapus axes kosong jika jumlah model kurang dari jumlah subplot\n",
    "for j in range(len(axes)):\n",
    "    if j >= num_models:\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "# Tambahkan judul utama dan tata letak\n",
    "fig.suptitle('Training and Validation Loss for Ensemble Models', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Sesuaikan ruang untuk judul utama\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Summary of Final Training and Validation Loss for Each Model ===\")\n",
    "for i, history in enumerate(ensemble_histories):\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Model {i+1:02d} - Final Loss: {final_train_loss:.4f}, Final Val Loss: {final_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights):\n",
    "        super(WeightedEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)  # Ganti nama dari self.weights\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = tf.stack([model(inputs, training=False) for model in self.models], axis=0)\n",
    "        return tf.tensordot(self.model_weights, outputs, axes=1)\n",
    "    \n",
    "# Hitung bobot berdasarkan val_loss\n",
    "weights = []\n",
    "valid_models = []\n",
    "valid_histories = []\n",
    "for model, history in zip(ensemble_models, ensemble_histories):\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    if not np.isnan(final_val_loss):\n",
    "        weights.append(1 / final_val_loss)\n",
    "        valid_models.append(model)\n",
    "        valid_histories.append(history)\n",
    "\n",
    "normalized_weights = np.array(weights) / np.sum(weights)\n",
    "ensemble_combined_model = WeightedEnsemble(valid_models, normalized_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict_probabilistic(models, X, histories):\n",
    "    valid_models = []\n",
    "    weights = []\n",
    "    for model, history in zip(models, histories):\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        if not np.isnan(final_val_loss):\n",
    "            valid_models.append(model)\n",
    "            weights.append(1 / final_val_loss)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    predictions = [model.predict(X) for model in valid_models]\n",
    "    weighted_predictions = np.average(predictions, axis=0, weights=normalized_weights)\n",
    "    std_predictions = np.sqrt(np.average((predictions - weighted_predictions) ** 2,\n",
    "                                         axis=0, weights=normalized_weights))\n",
    "    return weighted_predictions, std_predictions\n",
    "\n",
    "def calculate_rmse(y_actual, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "\n",
    "def calculate_picp(y_actual, mean_pred, delta):\n",
    "    lower_bound = mean_pred - delta\n",
    "    upper_bound = mean_pred + delta\n",
    "    within_interval = np.logical_and(y_actual >= lower_bound, y_actual <= upper_bound)\n",
    "    return np.mean(within_interval)\n",
    "\n",
    "def calculate_nmpiw(y_actual, delta):\n",
    "    interval_width = 2 * delta\n",
    "    return np.mean(interval_width) / (np.max(y_actual) - np.min(y_actual))\n",
    "\n",
    "mean_pred, std_pred = ensemble_predict_probabilistic(ensemble_models, X_test, ensemble_histories)\n",
    "z = 1.96\n",
    "delta = z * std_pred\n",
    "\n",
    "rmse = calculate_rmse(y_actual, mean_pred)\n",
    "picp = calculate_picp(y_actual, mean_pred, delta)\n",
    "nmpiw = calculate_nmpiw(y_actual, delta)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"PICP: {picp}\")\n",
    "print(f\"NMPIW: {nmpiw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model gabungan\n",
    "\n",
    "ensemble_combined_model.save(f'models_{n_models}_{subset}_{formatted}/{round(rmse, 4)}_{round(picp, 4)}_{round(nmpiw, 4)}_ensemble_model_combined.h5')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
